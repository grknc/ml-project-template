{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00",
   "metadata": {},
   "source": "# MovieLens Recommendation System\n\nBuild a collaborative filtering recommendation system for movies.\n\n**Dataset:** [https://www.kaggle.com/datasets/grouplens/movielens-20m-dataset](https://www.kaggle.com/datasets/grouplens/movielens-20m-dataset)  \n**Columns:** `userId`, `movieId`, `rating`  \n**Type:** Collaborative Filtering\n\n> **TODO:** Download the dataset, place it in `../../data/raw/`, then update `DATA_PATH` below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nsns.set_theme(style='whitegrid')"
  },
  {
   "cell_type": "markdown",
   "id": "c02",
   "metadata": {},
   "source": "## 1. Load Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03",
   "metadata": {},
   "outputs": [],
   "source": "DATA_PATH = \"../../data/raw/ratings.csv\"\nUSER_COL = \"userId\"\nITEM_COL = \"movieId\"\nRATING_COL = \"rating\"\n\ndf = pd.read_csv(DATA_PATH)\n# Use a sample for faster experimentation\ndf = df.sample(min(100_000, len(df)), random_state=42).reset_index(drop=True)\nprint(f'Shape: {df.shape}')\nprint(f'Users: {df[USER_COL].nunique()}, Items: {df[ITEM_COL].nunique()}')\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "id": "c04",
   "metadata": {},
   "source": "## 2. EDA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05",
   "metadata": {},
   "outputs": [],
   "source": "# Rating distribution\ndf[RATING_COL].value_counts().sort_index().plot(kind='bar', figsize=(8, 4))\nplt.title('Rating Distribution'); plt.xlabel('Rating')\nplt.tight_layout(); plt.show()\n\n# Ratings per user / item\nratings_per_user = df.groupby(USER_COL).size()\nratings_per_item = df.groupby(ITEM_COL).size()\nprint(f'Ratings per user: mean={ratings_per_user.mean():.1f}, median={ratings_per_user.median():.1f}')\nprint(f'Ratings per item: mean={ratings_per_item.mean():.1f}, median={ratings_per_item.median():.1f}')"
  },
  {
   "cell_type": "markdown",
   "id": "c06",
   "metadata": {},
   "source": "## 3. Train / Test Split"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07",
   "metadata": {},
   "outputs": [],
   "source": "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nprint(f'Train: {len(train_df)}, Test: {len(test_df)}')"
  },
  {
   "cell_type": "markdown",
   "id": "c08",
   "metadata": {},
   "source": "## 4. Popularity Baseline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09",
   "metadata": {},
   "outputs": [],
   "source": "# Recommend top-K most popular items to everyone\nK = 10\npopular_items = (\n    train_df.groupby(ITEM_COL)[RATING_COL].mean()\n    .sort_values(ascending=False)\n    .head(K)\n    .index.tolist()\n)\nprint(f'Top-{K} popular items: {popular_items[:5]}...')"
  },
  {
   "cell_type": "markdown",
   "id": "c10",
   "metadata": {},
   "source": "## 5. User-Item Matrix"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11",
   "metadata": {},
   "outputs": [],
   "source": "# Build matrix (may be large — filter active users/items for experiments)\nmin_ratings_user = 5\nmin_ratings_item = 5\nactive_users = ratings_per_user[ratings_per_user >= min_ratings_user].index\npopular_items_all = ratings_per_item[ratings_per_item >= min_ratings_item].index\nfiltered = train_df[\n    train_df[USER_COL].isin(active_users) & train_df[ITEM_COL].isin(popular_items_all)\n]\n\nuser_item_matrix = filtered.pivot_table(\n    index=USER_COL, columns=ITEM_COL, values=RATING_COL, fill_value=0\n)\nsparsity = 1 - (filtered.shape[0] / (user_item_matrix.shape[0] * user_item_matrix.shape[1]))\nprint(f'Matrix shape: {user_item_matrix.shape}')\nprint(f'Sparsity: {sparsity:.1%}')"
  },
  {
   "cell_type": "markdown",
   "id": "c12",
   "metadata": {},
   "source": "## 6. User-Based Collaborative Filtering"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13",
   "metadata": {},
   "outputs": [],
   "source": "user_sim = cosine_similarity(user_item_matrix)\nuser_sim_df = pd.DataFrame(\n    user_sim, index=user_item_matrix.index, columns=user_item_matrix.index\n)\n\ndef recommend_user_based(user_id, n=10):\n    if user_id not in user_sim_df.index:\n        return popular_items[:n]  # cold-start fallback\n    similar_users = user_sim_df[user_id].sort_values(ascending=False)[1:21].index\n    seen = set(user_item_matrix.loc[user_id][user_item_matrix.loc[user_id] > 0].index)\n    scores = user_item_matrix.loc[similar_users].mean().drop(index=list(seen), errors='ignore')\n    return scores.nlargest(n).index.tolist()\n\nsample_user = user_item_matrix.index[0]\nrecs = recommend_user_based(sample_user)\nprint(f'Recommendations for user {sample_user}: {recs}')"
  },
  {
   "cell_type": "markdown",
   "id": "c14",
   "metadata": {},
   "source": "## 7. Item-Based Collaborative Filtering"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15",
   "metadata": {},
   "outputs": [],
   "source": "item_sim = cosine_similarity(user_item_matrix.T)\nitem_sim_df = pd.DataFrame(\n    item_sim, index=user_item_matrix.columns, columns=user_item_matrix.columns\n)\n\ndef recommend_item_based(user_id, n=10):\n    if user_id not in user_item_matrix.index:\n        return popular_items[:n]\n    rated = user_item_matrix.loc[user_id]\n    rated_items = rated[rated > 0].index\n    scores = item_sim_df[rated_items].mean(axis=1)\n    scores = scores.drop(index=rated_items, errors='ignore')\n    return scores.nlargest(n).index.tolist()\n\nrecs_item = recommend_item_based(sample_user)\nprint(f'Item-based recs for user {sample_user}: {recs_item}')"
  },
  {
   "cell_type": "markdown",
   "id": "c16",
   "metadata": {},
   "source": "## 8. Offline Evaluation (Precision@K)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17",
   "metadata": {},
   "outputs": [],
   "source": "def precision_at_k(recommend_fn, test_df, k=10, n_users=200):\n    test_users = test_df[USER_COL].unique()[:n_users]\n    precisions = []\n    for uid in test_users:\n        relevant = set(test_df[test_df[USER_COL] == uid][ITEM_COL])\n        if not relevant:\n            continue\n        recs = set(recommend_fn(uid, n=k))\n        precisions.append(len(recs & relevant) / k)\n    return np.mean(precisions) if precisions else 0.0\n\np_user = precision_at_k(recommend_user_based, test_df)\np_item = precision_at_k(recommend_item_based, test_df)\nprint(f'User-based Precision@10: {p_user:.4f}')\nprint(f'Item-based Precision@10: {p_item:.4f}')"
  },
  {
   "cell_type": "markdown",
   "id": "c18",
   "metadata": {},
   "source": "## 9. Conclusion\n\n| Method | Precision@10 |\n|---|---|\n| Popularity Baseline | — |\n| User-Based CF | *(fill)* |\n| Item-Based CF | *(fill)* |\n\n**Observations:**\n- \n\n**Next steps:**\n- Try matrix factorization (SVD via `surprise` library)\n- Add content-based features (genres, tags)\n- Address cold-start with hybrid approach"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}